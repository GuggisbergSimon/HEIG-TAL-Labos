{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\" align=\"right\" /> \n",
    "\n",
    "# TAL Labo 1b : Segmentation de textes avec NLTK\n",
    "\n",
    "**Objectifs**\n",
    "\n",
    "Le but de cette deuxième partie du Labo 1 du Cours TAL est d'effectuer quelques opérations élémentaires sur les textes en utilisant la boîte à outils [NLTK](http://www.nltk.org/) en Python.  Vous utiliserez l'environnement mis en place dans la partie 1a : [Python 3](https://www.python.org/) avec _notebooks_ [Jupyter](https://jupyter.org/), soit localement sur votre ordinateur (avec ou sans Conda) ou en ligne sur [Google Colab](https://colab.research.google.com).\n",
    "\n",
    "Vous utiliserez NLTK pour obtenir des textes en ligne ou localement, puis vous les segmenterez en phrases et en mots (appelés aussi _tokens_).  Vous calculerez aussi des statistiques sur ces textes.  Vous travaillerez d'abord sur des textes en anglais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK: Natural Language Toolkit\n",
    "\n",
    "Pour ajouter NLTK à votre installation locale de Python, suivez les instructions sur le [site web NLTK](http://www.nltk.org/install.html).  Sur Google Colab, NLTK est déjà installé.\n",
    "\n",
    "Rappels de Python: pour utiliser NLTK en Python, notamment dans un _notebook_ Jupyter, vous devez exécuter `import nltk`, ce qui vous permet d'accéder à toutes les commandes avec le préfixe `nltk`.  Si vous écrivez `from nltk.book import *`, cela importera des commandes et des variables (p.ex. une collection de textes) qui seront accessibles sans utiliser le préfixe. \n",
    "\n",
    "**Remarques**\n",
    "* Le but de ce laboratoire est de vous initier à NLTK.  Vous pouvez aussi parcourir le [Chapitre 1](http://www.nltk.org/book/ch01.html) du [livre NLTK (*NLP with Python*)](http://www.nltk.org/book/) et essayer les commandes indiquées.  \n",
    "* Veuillez noter que le [livre en ligne](http://www.nltk.org/book/) est mis à jour pour Python 3, mais la [version imprimée](http://shop.oreilly.com/product/9780596516499.do) que l'on peut parfois trouver en PDF est pour Python 2. \n",
    "* NLTK inclut un gestionnaire de téléchargements (faire `import nltk` puis `nltk.download()`).  Cela vous permettra de télécharger de nombreux corpus additionnels, mais que nous n'utiliserons pas dans ce laboratoire."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:19:17.855225Z",
     "start_time": "2025-02-20T13:19:17.424007Z"
    }
   },
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "#from nltk.book import *\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Écrivez une liste de mots (strings) que vous appelerez `sentence1`, formant une phrase en anglais.\n",
    "\n",
    "Affichez sa longueur avec `len()`.  \n",
    "\n",
    "Utilisez `nltk.bigrams` pour générer tous les bi-grammes (couples de mots adjacents) à partir de cette liste.  Un exemple est montré dans la [section 3.3 du chapitre 1 du livre NLTK](http://www.nltk.org/book/ch01.html#collocations-and-bigrams).  \n",
    "\n",
    "Enfin, triez les bi-grammes par ordre alphabétique et affichez le résultat."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T12:52:23.466547Z",
     "start_time": "2025-02-20T12:52:23.446813Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "sentence1 = [\"this\", \"is\", \"a\", \"sentence\"]\n",
    "print(len(sentence1))\n",
    "bigrams = nltk.bigrams(sentence1)\n",
    "sorted(bigrams)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 'sentence'), ('is', 'a'), ('this', 'is')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** Définissez maintenant une chaîne unique appelée `string2` contenant une phrase, incluant par exemple des ponctuations.  \n",
    "\n",
    "Utilisez le *tokenizer* de NLTK (fonction `nltk.word_tokenize`, expliquée dans la [section 3.1 du livre NLTK](http://www.nltk.org/book/ch03.html#sec-accessing-text)) pour segmenter la chaîne en mots, qui seront rassemblés dans une liste que vous appelerez `sentence2`.  Affichez cette liste."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T12:52:26.824819Z",
     "start_time": "2025-02-20T12:52:26.690469Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "string2 = \"This is, for example, a sentence with punctuation.\"\n",
    "sentence2 = nltk.word_tokenize(string2)\n",
    "print(sentence2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', ',', 'for', 'example', ',', 'a', 'sentence', 'with', 'punctuation', '.']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## 2. Utiliser NLTK pour télécharger, segmenter et sauvegarder un texte\n",
    "\n",
    "**2a.** Inspirez-vous du [chapitre 3 (3.1. Processing Raw Text) du livre NLTK](http://www.nltk.org/book/ch03.html) pour télécharger un fichier avec du texte en ligne, par exemple un livre anglais du Projet Gutenberg.  Stockez son contenu dans une chaîne et affichez sa longueur."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T12:54:24.501443Z",
     "start_time": "2025-02-20T12:54:23.359485Z"
    }
   },
   "source": [
    "from urllib import request\n",
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "url = \"https://www.gutenberg.org/files/2701/2701-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "len(raw)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1240967"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** En inspectant la chaîne précédente, déterminez quelle partie du début et de la fin il faut enlever pour garder uniquement le texte principal du roman.  Vous pouvez utiliser la notation dite de *slicing* en Python, et essayer de localiser des chaînes qui indiquent où commence et finit le véritable texte. Quelle est la longueur de ce texte en caractères ?  \n",
    "\n",
    "Si vous avez des problèmes avec les retours à la ligne, pensez à les remplacer avec la fonction `.replace(..., ...)`.  Si vous avez des problèmes d'encodage, voir le [support d'Unicode dans Python](https://docs.python.org/3.7/howto/unicode.html)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T12:56:03.141051Z",
     "start_time": "2025-02-20T12:56:03.092251Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "trimmed = raw[59:-55]\n",
    "trimmed = trimmed.replace('\\r\\n', ' ').replace('\\r', ' ').replace('\\n', ' ')"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** Veuillez maintenant segmenter le texte en phrases avec NLTK, et afficher le nombre de phrases obtenues, ainsi qu'un court extrait de 4 phrases. \n",
    "\n",
    "Puis veuillez écrire les phrases dans un nouveau fichier, avec une phrase par ligne.\n",
    "\n",
    "Comment appréciez-vous la qualité de la segmentation ?  Veuillez écrire votre appréciation dans une nouvelle cellule ci-dessous.\n",
    "\n",
    "**Indications :**\n",
    "* si certains caractères spéciaux vous semblent péjorer la segmentation, vous pouvez les remplacer dans la chaîne globale avec la fonction `.replace('s1', 's2')`\n",
    "* vous aurez besoin de la fonction de nltk (vue en cours) appelée `nltk.sent_tokenize(...)` qui est documentée [ici](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize) (rappel : le nom \"sentence tokenize\" n'est pas très logique)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-20T12:56:06.774517Z",
     "start_time": "2025-02-20T12:56:06.084150Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "sentences = nltk.sent_tokenize(trimmed)\n",
    "print(len(sentences))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9063\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T12:56:08.527561Z",
     "start_time": "2025-02-20T12:56:08.493967Z"
    }
   },
   "source": [
    "filename1 = \"sample_text_1.txt\"\n",
    "# Pour un fichier local : chemin relatif par rapport au notebook\n",
    "# Pour Google Colab, p.ex.: /content/gdrive/My Drive/sample_text_1.txt\n",
    "if os.path.exists(filename1): \n",
    "    os.remove(filename1)\n",
    "fd = open(filename1, 'a', encoding='utf8')\n",
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "fd.write('\\n'.join(sentences))\n",
    "fd.close()"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T12:59:27.411052Z",
     "start_time": "2025-02-20T12:59:27.392819Z"
    }
   },
   "source": [
    "# Votre appréciation de la qualité ici :\n",
    "# Après remplacement des `/n/r` par un ` `, la majeure partie des phrases a une grande qualité mais certaines \"phrases\" n'en sont en fait pas, tel que la table des matières des chapitres.\n",
    "# De plus le livre contient une forme de mise en page et des charactères spéciaux qui rendent la lecture, avant ou après segmentation, discutable, par exemple :\n",
    "print(sentences[289])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Wallen_;   A.S. _Walw-ian_, to roll, to wallow.” —_Richardson’s Dictionary._     חו,                 _Hebrew_.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d.** Veuillez maintenant segmenter chaque phrase du (2c) en tokens (mots et ponctuations).  Stockez le résultat dans une nouvelle variable (liste de listes).  Affichez le nombre total de _tokens_. Puis, affichez 4 phrases et commentez la qualité de la tokenisation.  Veuillez également créer un autre fichier, avec une phrase par ligne, et un espace entre chaque _token_.  \n",
    "\n",
    "Vous aurez besoin de la fonction de nltk (vue en cours) appelée `nltk.word_tokenize(...)` (documentée [ici](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize))."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:03:50.473275Z",
     "start_time": "2025-02-20T13:03:46.945846Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "print(sum(len(sentence) for sentence in tokenized_sentences))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255958\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:06:16.651203Z",
     "start_time": "2025-02-20T13:06:16.639902Z"
    }
   },
   "source": [
    "# Votre appréciation de la qualité ici :\n",
    "# La tokénaization est de bonne qualité, les mots et les ponctuations sont bien séparés, par un espace, afin de simular l'affichage de la liste de liste. Exemple de 4 phrases :\n",
    "for sentence in tokenized_sentences[500:504]:\n",
    "    print(' '.join(sentence))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep into distant woodlands winds a mazy way , reaching to overlapping spurs of mountains bathed in their hill-side blue .\n",
      "But though the picture lies thus tranced , and though this pine-tree shakes down its sighs like leaves upon this shepherd ’ s head , yet all were vain , unless the shepherd ’ s eye were fixed upon the magic stream before him .\n",
      "Go visit the Prairies in June , when for scores on scores of miles you wade knee-deep among Tiger-lilies—what is the one charm wanting ? —Water—there is not a drop of water there !\n",
      "Were Niagara but a cataract of sand , would you travel your thousand miles to see it ?\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:08:04.605068Z",
     "start_time": "2025-02-20T13:08:04.528847Z"
    }
   },
   "source": [
    "filename2 = \"sample_text_2.txt\"\n",
    "# Pour un fichier local : chemin relatif par rapport au notebook\n",
    "# Pour Google Colab, p.ex.: /content/gdrive/My Drive/sample_text_2.txt\n",
    "if os.path.exists(filename2): \n",
    "    os.remove(filename2)\n",
    "fd = open(filename2, 'a', encoding='utf8')\n",
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "for sentence in tokenized_sentences:\n",
    "    fd.write(' '.join(sentence) + '\\n')\n",
    "fd.close()"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2e.** Veuillez maintenant tokeniser un texte sans le segmenter préalablement en phrases, en effectuant la tokenisation directement sur la chaîne de caractères contenant tout le texte.  Veuillez afficher un court extrait de 50 tokens. \n",
    "\n",
    "Veuillez afficher le nombre total de tokens : est-ce qu'il est identique au total obtenu au (2d) ?  \n",
    "\n",
    "Il n'est pas demandé ici d'écrire le résultat dans un fichier."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:09:47.638864Z",
     "start_time": "2025-02-20T13:09:44.248200Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "tokenized_all = nltk.word_tokenize(trimmed)\n",
    "print(len(tokenized_all))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255958\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:12:45.487722Z",
     "start_time": "2025-02-20T13:12:45.477134Z"
    }
   },
   "source": [
    "# Votre réponse à la question ici :\n",
    "# Oui, le nombre de token obtenu est similaire à la question 2d, est encore heureux. On perd la notion de phrase avec cette approche par contre, mais celle-ci pourrait être retrouvée, en utilisant la séparation en phrases par un `.` par exemple."
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculer diverses statistiques avec NLTK\n",
    "Pour calculer des statistiques, il faut d'abord créer un objet de type `nltk.Text`.  \n",
    "\n",
    "Les méthodes `nltk.word_tokenize()` et `nltk.sent_tokenize()` s'appliquent à des chaînes, pas des `nltk.Text`.\n",
    "\n",
    "Les objets `nltk.Text` peuvent en principe être créés avec : \n",
    "1. la chaîne de caractères constituant le texte (string)\n",
    "2. la liste de tous les mots du texte (tableau de string)\n",
    "3. la liste de toutes les phrases (tableau de listes de string)\n",
    "\n",
    "**Seule l'option (2) permet d'utiliser correctement les méthodes de `nltk.Text`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a.** Créez et stockez dans une variable un objet `nltk.Text` à partir de la liste des tokens de votre texte.  Vous pouvez appeler ici de nouveau la fonction `nltk.word_tokenize` ou réutiliser le résultat du (2e).  Il n'y a rien à afficher ici."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:12:59.363938Z",
     "start_time": "2025-02-20T13:12:59.347193Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "nltkText = nltk.Text(tokenized_all)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Veuillez afficher les occurrences d'un mot et leur contexte immédiat avec la méthode `concordance`.  Cette méthode est décrite au\n",
    " [chapitre 1 du livre NLTK](http://www.nltk.org/book/ch01.html) qui montre aussi d'autres exemples d'opérations que l'on peut effectuer sur un objet `nltk.Text`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:20:14.996133Z",
     "start_time": "2025-02-20T13:20:14.986110Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "nltkText.concordance('ishmael')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 19 of 19 matches:\n",
      "g_ . CHAPTER 1 . Loomings . Call me Ishmael . Some years ago—never mind how lon\n",
      "ed States._ “ WHALING VOYAGE BY ONE ISHMAEL . “ BLOODY BATTLE IN AFFGHANISTAN. \n",
      "of silver , —So , wherever you go , Ishmael , said I to myself , as I stood in \n",
      "de to lodge for the night , my dear Ishmael , be sure to inquire the price , an\n",
      "nkling glasses within . But go on , Ishmael , said I at last ; don ’ t you hear\n",
      "ing and teeth-gnashing there . Ha , Ishmael , muttered I , backing out , Wretch\n",
      "emen who had gone before me . Yes , Ishmael , the same fate may be thine . But \n",
      " ? thought I . Do you suppose now , Ishmael , that the magnanimous God of heave\n",
      "l , which , if left to myself , I , Ishmael , should infallibly light upon , fo\n",
      " Bildad . Now then , my young man , Ishmael ’ s thy name , didn ’ t ye say ? We\n",
      "say ? Well then , down ye go here , Ishmael , for the three hundredth lay. ” “ \n",
      "fear ! CHAPTER 41 . Moby Dick . I , Ishmael , was one of that crew ; my shouts \n",
      "lain , would be to dive deeper than Ishmael can go . The subterranean miner tha\n",
      "oul ; thou surrenderest to a hypo , Ishmael . Tell me , why this strong young c\n",
      " snows of prairies ; all these , to Ishmael , are as the shaking of that buffal\n",
      "ubtle meanings , how may unlettered Ishmael hope to read the awful Chaldee of t\n",
      "onditional skeleton . But how now , Ishmael ? How is it , that you , a mere oar\n",
      " for exhibition ? Explain thyself , Ishmael . Can you land a full-grown whale o\n",
      "le witness have you hitherto been , Ishmael ; but have a care how you seize the\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c.** Veuillez trouver les 10 mots qui ont les contextes les plus semblables à un mot de votre choix, en utilisant la méthode `similar`.  Cette méthode est aussi décrite au [chapitre 1 du livre NLTK](http://www.nltk.org/book/ch01.html).  Est-ce que les mots ayant des contextes semblables sont aussi semblables par le sens au mot choisi ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:16:14.171884Z",
     "start_time": "2025-02-20T13:16:13.911566Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "nltkText.similar('sea')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whale ship deck world pequod other air boat captain time water crew\n",
      "line whales last way wind night seas head\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:18:36.558172Z",
     "start_time": "2025-02-20T13:18:36.551958Z"
    }
   },
   "source": [
    "# Votre réponse à la question ici :\n",
    "# Les mots ayant des contextes semblables ne sont pas forcément semblables par le sens au mot choisi. Par exemple \"sea\" a des contextes similaires comme \"water\" mais \"wind\" ou \"night\", qui ne matchent qu'au niveau du contexte, pas au niveau du sens."
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d.** En utilisant la méthode `collocation_list`, veuilez afficher les 10 collocations (couples de mots) les plus fréquentes dans votre texte."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:19:37.935763Z",
     "start_time": "2025-02-20T13:19:36.619959Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "nltkText.collocation_list()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sperm', 'Whale'),\n",
       " ('Moby', 'Dick'),\n",
       " ('White', 'Whale'),\n",
       " ('old', 'man'),\n",
       " ('Right', 'Whale'),\n",
       " ('Captain', 'Ahab'),\n",
       " ('sperm', 'whale'),\n",
       " ('New', 'Bedford'),\n",
       " ('Captain', 'Peleg'),\n",
       " ('Mr.', 'Starbuck'),\n",
       " ('Cape', 'Horn'),\n",
       " ('cried', 'Ahab'),\n",
       " ('Mrs.', 'Hussey'),\n",
       " ('years', 'ago'),\n",
       " ('chief', 'mate'),\n",
       " ('lower', 'jaw'),\n",
       " ('Father', 'Mapple'),\n",
       " ('white', 'whale'),\n",
       " ('ivory', 'leg'),\n",
       " ('cried', 'Stubb')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3e.** On peut déterminer le vocabulaire d'un texte (c'est-à-dire la liste de ses _types_) simplement en convertissant la liste des _tokens_ déjà obtenue dans une variable de type `set` en Python.  \n",
    "* Veuillez obtenir ainsi le vocabulaire de votre texte.\n",
    "* Combien de mots différents y a-t-il dans le vocabulaire, en incluant les ponctuations et tout autre symbole ? \n",
    "* Quels sont les 20 types les plus longs ? Que pensez-vous du résultat trouvé ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:25:20.729479Z",
     "start_time": "2025-02-20T13:25:20.667436Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "tokenized_all_set = set(tokenized_all)\n",
    "print(len(tokenized_all_set))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21897\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:26:19.246480Z",
     "start_time": "2025-02-20T13:26:19.221461Z"
    }
   },
   "source": [
    "# Votre réponse à la question ici :\n",
    "print(sorted(tokenized_all_set, key=len, reverse=True)[:20])\n",
    "# Les 20 mots les plus longs sont ceux composés d'un tirait (dash) ce qui n'est pas vraiment représentatif d'un vrai mot anglais. La raison à un usage stylistique."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['swayings—coyings—flutterings', 'ocean-perishing—straight', 'standers-of-mast-heads', 'Forty-barrel-bull—poor', 'tastefully-ornamented', 'immortality-preserver', 'surface—involuntarily', 'stander-of-mast-heads', 'East-south-east—which', 'exclamation-like—that', 'time—queer—sir—queer', 'astrological-looking', 'country-schoolmaster', 'cruel—forbidding—now', 'Daggoo—instinctively', 'uninterpenetratingly', 'deceptive—spoutings', 'line-of-battle-ship', 'twisted—corkscrewed', 'blows—bowes—bo-o-os']\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3f.** Veuillez construire la distribution de fréquences de votre texte, en utilisant un objet `FreqDist`, et utilisez-là pour afficher les mots qui ont plus de 4 lettres parmi les 70 mots les plus fréquents.\n",
    "\n",
    "Indication : NLTK peut facilement calculer les fréquences de tous les _tokens_ dans un `nltk.Text` en les stockant dans un objet de type `FreqDist` (pour _frequency distribution_) comme expliqué dans la [section 3.1 du chapitre 1 du livre NLTK](http://www.nltk.org/book/ch01.html#frequency-distributions).  À partir de cet objet, on peut déterminer les mots les plus fréquents."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T13:28:56.841069Z",
     "start_time": "2025-02-20T13:28:56.024240Z"
    }
   },
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    "freqDistTokens = nltk.FreqDist(tokenized_all)\n",
    "freqDistTokens.most_common(20)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19211),\n",
       " ('the', 13717),\n",
       " ('.', 7164),\n",
       " ('of', 6563),\n",
       " ('and', 6009),\n",
       " ('to', 4515),\n",
       " ('a', 4507),\n",
       " (';', 4178),\n",
       " ('in', 3915),\n",
       " ('that', 2918),\n",
       " ('’', 2790),\n",
       " ('his', 2457),\n",
       " ('it', 2153),\n",
       " ('I', 2074),\n",
       " ('s', 1785),\n",
       " ('!', 1767),\n",
       " ('is', 1686),\n",
       " ('with', 1661),\n",
       " ('he', 1635),\n",
       " ('was', 1625)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graphiques\n",
    "La librairie `matplotlib` permet d'afficher les statistiques des textes sous forme graphique.  Pour la rendre accessible dans le _notebook_ (une fois installée dans Python), il faut exécuter les deux lignes suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4a.** Veuillez afficher le graphique cumulatif des nombres d'occurrences des 70 mots les plus fréquents de votre texte, en vous inspirant de la  [section 3.1 du chapitre 1 du livre NLTK](http://www.nltk.org/book/ch01.html#frequency-distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.** Veuillez construir une liste avec la longueur de chaque _token_ du texte.  La liste aura donc autant de nombres que de tokens.  Veuillez créer un nouvel objet `FreqDist` à partir de cette liste de nombres, et affichez la distribution _non-cumulative_ des nombres d'occurrences.  Quelle est la longueur la plus fréquente ?  Comment évolue la longueur en fonction de la fréquence ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vos réponses aux deux questions ici :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c.** Veuillez générer la liste des fréquences des mots de votre texte par ordre décroissant (sans les mots, seulement les valeurs des fréquences).  Limitez cette liste à *N* valeurs (par exemple *N* = 100).  Affichez avec `matplotlib.pyplot.plot` la courbe en fonction du rang, c'est-à-dire le rang (1, 2, 3, ..., **) sur l'axe *x* et la fréquence sur l'axe *y*.\n",
    "\n",
    "Note : on génère directement des graphiques à partir de deux listes `x_values` et `y_values` avec la commande `matplotlib.pyplot.plot(x_values, y_values)`.\n",
    "\n",
    "Ajoutez une deuxième courbe (dans la même commande `plot`) selon la formule *y* = *a* / (*x* + *b*) en choississant par essais successifs des valeurs de *a* et *b* qui vous rapprochent autant que possible de la courbe des fréquences.  Cette formule est appelée *Loi de Zipf* et illustre une propriété du vocabulaire d'un échantillon suffisamment grand de texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire votre code ci-dessous, puis exécuter cette cellule.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin de la partie 1b\n",
    "Veuillez nettoyer autant que possible ce _notebook_, exécutez une dernière fois toutes les cellules pour obtenir les résultats demandés, et enregistrez le _notebook_ en ajoutant vos deux noms.  Puis ajoutez-le dans une archive _zip_ avec le _notebook_ de la partie 1c, et soumettez l'archive sur Cyberlearn. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
