{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Labo 5 Modèle word2vec et ses applications",
   "id": "cbcc0bd83bb1f042"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Tester et évaluer un modèle déjà entraîné sur Google News\n",
    "\n",
    "Veuillez télécharger le modèle word2vec pré-entraîné sur le corpus Google News en écrivant :\n",
    "```python\n",
    "from gensim import downloader as api\n",
    "w2v_vectors = api.load(\"word2vec-google-news-300\")\n",
    "```\n",
    "ce qui téléchargera le fichier la première fois.\n",
    "Après avoir téléchargé le modèle, vous pourrez l’utiliser ainsi (dans le dossier gensim-data) :\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "w2v_vectors = KeyedVectors.load_word2vec_format(path_to_file, binary=True)\n",
    "```"
   ],
   "id": "9c7fbbc1e9dccc7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Takes ~30min to download. Only execute once\n",
    "from gensim import downloader as api\n",
    "w2v_vectors = api.load(\"word2vec-google-news-300\")"
   ],
   "id": "4530a77693e4874d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T15:47:47.281192Z",
     "start_time": "2025-05-06T15:47:47.274988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model is then located at home directory, adjust as needed.\n",
    "import os\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"gensim-data\", \"word2vec-google-news-300\", \"word2vec-google-news-300.gz\")"
   ],
   "id": "8647e6f564f562f0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T15:48:54.915517Z",
     "start_time": "2025-05-06T15:47:53.223796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Takes ~1min\n",
    "from gensim.models import KeyedVectors\n",
    "w2v_vectors = KeyedVectors.load_word2vec_format(file_path, binary=True)"
   ],
   "id": "d45bdf54f53e0683",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### a. Quelle place en mémoire occupe le processus du notebook avec les vecteurs de mots ?",
   "id": "e2d924d3db2ec796"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:19:44.094296Z",
     "start_time": "2025-05-06T14:19:44.085178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "# Get the current process ID\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "# Get memory information in bytes and convert to MB\n",
    "memory_info = process.memory_info()\n",
    "memory_usage_mb = memory_info.rss / 1024 / 1024  # Convert to MB\n",
    "\n",
    "print(f\"Memory usage of the notebook process: {memory_usage_mb:.2f} MB\")\n",
    "print(f\"Word2vec model size: {w2v_vectors.vectors.nbytes / 1024 / 1024:.2f} MB\")"
   ],
   "id": "8fc958485998331c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of the notebook process: 4003.99 MB\n",
      "Word2vec model size: 3433.23 MB\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### b. Quelle est la dimension de l’espace vectoriel dans lequel les mots sont représentés ?",
   "id": "89510d7b01319c03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:18:59.373657Z",
     "start_time": "2025-05-06T14:18:59.368336Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Vector dimension: {w2v_vectors.vector_size}\")",
   "id": "beaa4ea8c5d8a78e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector dimension: 300\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### c. Quelle est la taille du vocabulaire connu du modèle ? Veuillez afficher cinq mots anglais qui sont dans le vocabulaire et deux qui ne le sont pas.\n",
   "id": "79739b63d4205b39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:25:09.221419Z",
     "start_time": "2025-05-06T14:25:09.212884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get vocabulary size\n",
    "vocab_size = len(w2v_vectors)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Five words in vocabulary\n",
    "words_in_vocab = ['baste', 'spleen', 'maudlin', 'saudade', 'aa']\n",
    "print(\"\\nFive words in vocabulary:\")\n",
    "for word in words_in_vocab:\n",
    "    if word in w2v_vectors:\n",
    "        print(f\"'{word}' is in vocabulary\")\n",
    "    else:\n",
    "        print(f\"'{word}' is NOT in vocabulary\")\n",
    "\n",
    "# Two words not in vocabulary\n",
    "words_not_in_vocab = ['enneahedron', 'petrichor']\n",
    "print(\"\\nTwo words not in vocabulary:\")\n",
    "for word in words_not_in_vocab:\n",
    "    if word in w2v_vectors:\n",
    "        print(f\"'{word}' is in vocabulary\")\n",
    "    else:\n",
    "        print(f\"'{word}' is NOT in vocabulary\")"
   ],
   "id": "e98e17edd81a3fca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3000000\n",
      "\n",
      "Five words in vocabulary:\n",
      "'baste' is in vocabulary\n",
      "'spleen' is in vocabulary\n",
      "'maudlin' is in vocabulary\n",
      "'saudade' is in vocabulary\n",
      "'aa' is in vocabulary\n",
      "\n",
      "Two words not in vocabulary:\n",
      "'enneahedron' is NOT in vocabulary\n",
      "'petrichor' is NOT in vocabulary\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### d. Quelle est la similarité entre les mots rabbit et carrot ? Veuillez rappeler comment on mesure les similarités entre deux mots grâce à leurs vecteurs.",
   "id": "1d3b8cf0b8f09c6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:27:01.610292Z",
     "start_time": "2025-05-06T14:27:01.605380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similarity = w2v_vectors.similarity('rabbit', 'carrot')\n",
    "print(f\"Similarity between 'rabbit' and 'carrot': {similarity:.4f}\")"
   ],
   "id": "3928ca04ad311dce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'rabbit' and 'carrot': 0.3631\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> TODO answer",
   "id": "cf72ee993cf593a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### e. Considérez au moins 5 paires de mots anglais, certains proches par leurs sens, d’autres plus éloignés. Pour chaque paire, calculez la similarité entre les deux mots. Veuillez indiquer si les similarités obtenues correspondent à vos intuitions sur la proximité des sens des mots.",
   "id": "8259a27d4140edbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:30:13.852502Z",
     "start_time": "2025-05-06T14:30:13.844755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pairs = [\n",
    "    ('king', 'queen'),\n",
    "    ('bad', 'bunny'),\n",
    "    ('good', 'bunny'),\n",
    "    ('plane', 'tower'),\n",
    "    ('duc', 'duchesse')\n",
    "]\n",
    "\n",
    "for pair in pairs:\n",
    "    word1, word2 = pair\n",
    "    if word1 in w2v_vectors and word2 in w2v_vectors:\n",
    "        similarity = w2v_vectors.similarity(word1, word2)\n",
    "        print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n",
    "    else:\n",
    "        print(f\"One or both words in the pair '{pair}' are not in the vocabulary.\")"
   ],
   "id": "76e8e48ac1c3d37d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': 0.6511\n",
      "Similarity between 'bad' and 'bunny': 0.1326\n",
      "Similarity between 'good' and 'bunny': 0.1150\n",
      "Similarity between 'plane' and 'tower': 0.2549\n",
      "Similarity between 'duc' and 'duchesse': 0.3245\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> TODO answer and/or pick better words",
   "id": "4951989ff91ba049"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### f. Pouvez-vous trouver des mots de sens opposés mais qui sont proches selon le modèle ? Comment expliquez-vous cela ? Est-ce une qualité ou un défaut du modèle word2vec ?",
   "id": "6df9c32c1020cc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:33:42.213054Z",
     "start_time": "2025-05-06T14:33:42.209169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similarity = w2v_vectors.similarity('black', 'white')\n",
    "print(f\"Similarity: {similarity:.4f}\")"
   ],
   "id": "b94799ffa7d27059",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.8092\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ">TODO answer",
   "id": "6fe7e73112415bfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### g. En vous aidant de la documentation de Gensim sur KeyedVectors, obtenez les scores du modèle word2vec sur les données de test WordSimilarity-353. Veuillez rappeler en 1-2 phrases comment les différents scores sont calculés.",
   "id": "91bab382b5159fd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:36:41.356661Z",
     "start_time": "2025-05-06T14:36:41.141005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "word_sim_353_path = datapath('wordsim353.tsv')\n",
    "result = w2v_vectors.evaluate_word_pairs(word_sim_353_path)\n",
    "\n",
    "print(f\"Pearson correlation coefficient: {result[0][0]:.4f}\")\n",
    "print(f\"Pearson correlation p-value: {result[0][1]:.4f}\")\n",
    "print(f\"Spearman rank correlation coefficient: {result[1][0]:.4f}\")\n",
    "print(f\"Spearman rank correlation p-value: {result[1][1]:.4f}\")\n",
    "print(f\"Ratio of pairs with OOV words: {result[2]:.2%}\")"
   ],
   "id": "3aa6ea7a573d85c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.6239\n",
      "Pearson correlation p-value: 0.0000\n",
      "Spearman rank correlation coefficient: 0.6589\n",
      "Spearman rank correlation p-value: 0.0000\n",
      "Ratio of pairs with OOV words: 0.00%\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> TODO answer",
   "id": "6e8da2fda5367591"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### h. En vous aidant de la documentation, calculez le score du modèle word2vec sur les données questions-words.txt. Attention, cette évaluation prend une dizaine de minutes, donc il vaut mieux commencer par tester avec un fragment de ce fichier (copier/coller les 100 premières analogies). Expliquez en 1-2 phrases comment ce score est calculé et ce qu’il mesure.",
   "id": "c3ec3e208878055c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T15:50:33.851209Z",
     "start_time": "2025-05-06T15:49:03.273206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.test.utils import datapath\n",
    "import time\n",
    "\n",
    "analogies_path = datapath('questions-words.txt')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "result = w2v_vectors.evaluate_word_analogies(analogies_path)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Total accuracy: {result[0]:.4f}\")\n",
    "print(f\"Number of sections: {len(result[1])}\")\n",
    "print(f\"Evaluation time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nSection-wise results:\")\n",
    "for section_name, section_result in result[1]:\n",
    "    print(f\"Section '{section_name}': {section_result[0]:.4f} accuracy ({section_result[1]} correct out of {section_result[2]} total)\")"
   ],
   "id": "18831db6d37e4dd3",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      4\u001B[39m analogies_path = datapath(\u001B[33m'\u001B[39m\u001B[33mquestions-words.txt\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      6\u001B[39m start_time = time.time()\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m result = \u001B[43mw2v_vectors\u001B[49m\u001B[43m.\u001B[49m\u001B[43mevaluate_word_analogies\u001B[49m\u001B[43m(\u001B[49m\u001B[43manalogies_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m elapsed_time = time.time() - start_time\n\u001B[32m     12\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTotal accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult[\u001B[32m0\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/HEIG/TAL/HEIG-TAL-Labos/.venv/lib/python3.12/site-packages/gensim/models/keyedvectors.py:1371\u001B[39m, in \u001B[36mKeyedVectors.evaluate_word_analogies\u001B[39m\u001B[34m(self, analogies, restrict_vocab, case_insensitive, dummy4unknown, similarity_function)\u001B[39m\n\u001B[32m   1367\u001B[39m predicted = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1368\u001B[39m \u001B[38;5;66;03m# find the most likely prediction using 3CosAdd (vector offset) method\u001B[39;00m\n\u001B[32m   1369\u001B[39m \u001B[38;5;66;03m# TODO: implement 3CosMul and set-based methods for solving analogies\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1371\u001B[39m sims = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmost_similar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpositive\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnegative\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43ma\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopn\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrestrict_vocab\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrestrict_vocab\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1372\u001B[39m \u001B[38;5;28mself\u001B[39m.key_to_index = original_key_to_index\n\u001B[32m   1373\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m element \u001B[38;5;129;01min\u001B[39;00m sims:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/HEIG/TAL/HEIG-TAL-Labos/.venv/lib/python3.12/site-packages/gensim/models/keyedvectors.py:855\u001B[39m, in \u001B[36mKeyedVectors.most_similar\u001B[39m\u001B[34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001B[39m\n\u001B[32m    852\u001B[39m best = matutils.argsort(dists, topn=topn + \u001B[38;5;28mlen\u001B[39m(all_keys), reverse=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    853\u001B[39m \u001B[38;5;66;03m# ignore (don't return) keys from the input\u001B[39;00m\n\u001B[32m    854\u001B[39m result = [\n\u001B[32m--> \u001B[39m\u001B[32m855\u001B[39m     (\u001B[38;5;28mself\u001B[39m.index_to_key[sim + clip_start], \u001B[38;5;28mfloat\u001B[39m(dists[sim]))\n\u001B[32m    856\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m sim \u001B[38;5;129;01min\u001B[39;00m best \u001B[38;5;28;01mif\u001B[39;00m (sim + clip_start) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m all_keys\n\u001B[32m    857\u001B[39m ]\n\u001B[32m    858\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result[:topn]\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Entraîner deux nouveaux modèles word2vec à partir de deux corpus",
   "id": "98b61a019db22f42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### a. En utilisant gensim.downloader (voir question 1) récupérez le corpus qui contient les 108 premiers caractères de Wikipédia (en anglais) avec la commande : corpus = api.load('text8'). Combien de phrases et de mots (tokens) possède ce corpus ?",
   "id": "9dfc3d1cc321d9c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8f64562501f2e437"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### b. Entraînez un nouveau modèle word2vec sur ce nouveau corpus (voir la documentation de Word2vec). Si nécessaire, procédez progressivement, en commençant par utiliser 1% du corpus, puis 10%, etc., pour contrôler le temps nécessaire.\n",
    "- Veuillez indiquer la dimension choisie pour le embedding de ce nouveau modèle.\n",
    "- Combien de temps prend l’entraînement sur le corpus total ?\n",
    "- Quelle est la taille (en Mo) du modèle word2vec résultant ?\n",
    "### c. Mesurez la qualité de ce modèle comme en (1g) et (1h). Ce modèle est-il meilleur que celui entraîné sur Google News ? Quelle est selon vous la raison de la différence ?\n",
    "### d. Téléchargez maintenant le corpus quatre fois plus grand constitué de la concaténation du corpus text8 et des dépêches économiques de Reuters fourni par l’enseignant et appelé wikipedia_augmented.zip (à décompresser en un fichier ‘.dat’ de 413 Mo). Entraînez un nouveau modèle word2vec sur ce corpus, en précisant la dimension choisie pour les embeddings.\n",
    "- Utilisez la classe Text8Corpus() pour charger ce corpus, ce qui fera automatiquement la\n",
    "tokenisation et la segmentation en phrases.\n",
    "- Combien de temps prend l’entraînement ?\n",
    "- Quelle est la taille (en Mo) du modèle word2vec résultant ?\n",
    "### e. Testez ce modèle comme en (1g) et (1h). Est-il meilleur que le précédent ?"
   ],
   "id": "6de4eeb259fd1f9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a4cc87605b005171"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
