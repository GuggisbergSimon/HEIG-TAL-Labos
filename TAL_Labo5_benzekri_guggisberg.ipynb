{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbcc0bd83bb1f042",
   "metadata": {},
   "source": [
    "# Labo 5 Modèle word2vec et ses applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7fbbc1e9dccc7b",
   "metadata": {},
   "source": [
    "## 1. Tester et évaluer un modèle déjà entraîné sur Google News\n",
    "\n",
    "Veuillez télécharger le modèle word2vec pré-entraîné sur le corpus Google News en écrivant :\n",
    "```python\n",
    "from gensim import downloader as api\n",
    "w2v_vectors = api.load(\"word2vec-google-news-300\")\n",
    "```\n",
    "ce qui téléchargera le fichier la première fois.\n",
    "Après avoir téléchargé le modèle, vous pourrez l’utiliser ainsi (dans le dossier gensim-data) :\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "w2v_vectors = KeyedVectors.load_word2vec_format(path_to_file, binary=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4530a77693e4874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Takes ~30min to download. Only execute once\n",
    "from gensim import downloader as api\n",
    "w2v_vectors = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8647e6f564f562f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T15:47:47.281192Z",
     "start_time": "2025-05-06T15:47:47.274988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model is then located at home directory, adjust as needed.\n",
    "import os\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"gensim-data\", \"word2vec-google-news-300\", \"word2vec-google-news-300.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45bdf54f53e0683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T15:48:54.915517Z",
     "start_time": "2025-05-06T15:47:53.223796Z"
    }
   },
   "outputs": [],
   "source": [
    "# Takes ~1min\n",
    "from gensim.models import KeyedVectors\n",
    "w2v_vectors = KeyedVectors.load_word2vec_format(file_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d924d3db2ec796",
   "metadata": {},
   "source": [
    "### a. Quelle place en mémoire occupe le processus du notebook avec les vecteurs de mots ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc958485998331c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:19:44.094296Z",
     "start_time": "2025-05-06T14:19:44.085178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of the notebook process: 1024.30 MB\n",
      "Word2vec model size: 3433.23 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "# Get the current process ID\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "# Get memory information in bytes and convert to MB\n",
    "memory_info = process.memory_info()\n",
    "memory_usage_mb = memory_info.rss / 1024 / 1024  # Convert to MB\n",
    "\n",
    "print(f\"Memory usage of the notebook process: {memory_usage_mb:.2f} MB\")\n",
    "print(f\"Word2vec model size: {w2v_vectors.vectors.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89510d7b01319c03",
   "metadata": {},
   "source": [
    "### b. Quelle est la dimension de l’espace vectoriel dans lequel les mots sont représentés ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beaa4ea8c5d8a78e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:18:59.373657Z",
     "start_time": "2025-05-06T14:18:59.368336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector dimension: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vector dimension: {w2v_vectors.vector_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79739b63d4205b39",
   "metadata": {},
   "source": [
    "### c. Quelle est la taille du vocabulaire connu du modèle ? Veuillez afficher cinq mots anglais qui sont dans le vocabulaire et deux qui ne le sont pas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e98e17edd81a3fca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:25:09.221419Z",
     "start_time": "2025-05-06T14:25:09.212884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3000000\n",
      "\n",
      "Five words in vocabulary:\n",
      "'baste' is in vocabulary\n",
      "'spleen' is in vocabulary\n",
      "'maudlin' is in vocabulary\n",
      "'saudade' is in vocabulary\n",
      "'aa' is in vocabulary\n",
      "\n",
      "Two words not in vocabulary:\n",
      "'enneahedron' is NOT in vocabulary\n",
      "'petrichor' is NOT in vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary size\n",
    "vocab_size = len(w2v_vectors)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Five words in vocabulary\n",
    "words_in_vocab = ['baste', 'spleen', 'maudlin', 'saudade', 'aa']\n",
    "print(\"\\nFive words in vocabulary:\")\n",
    "for word in words_in_vocab:\n",
    "    if word in w2v_vectors:\n",
    "        print(f\"'{word}' is in vocabulary\")\n",
    "    else:\n",
    "        print(f\"'{word}' is NOT in vocabulary\")\n",
    "\n",
    "# Two words not in vocabulary\n",
    "words_not_in_vocab = ['enneahedron', 'petrichor']\n",
    "print(\"\\nTwo words not in vocabulary:\")\n",
    "for word in words_not_in_vocab:\n",
    "    if word in w2v_vectors:\n",
    "        print(f\"'{word}' is in vocabulary\")\n",
    "    else:\n",
    "        print(f\"'{word}' is NOT in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b8cf0b8f09c6c",
   "metadata": {},
   "source": [
    "### d. Quelle est la similarité entre les mots rabbit et carrot ? Veuillez rappeler comment on mesure les similarités entre deux mots grâce à leurs vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3928ca04ad311dce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:27:01.610292Z",
     "start_time": "2025-05-06T14:27:01.605380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'rabbit' and 'carrot': 0.3631\n"
     ]
    }
   ],
   "source": [
    "similarity = w2v_vectors.similarity('rabbit', 'carrot')\n",
    "print(f\"Similarity between 'rabbit' and 'carrot': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72ee993cf593a2",
   "metadata": {},
   "source": [
    "> TODO answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259a27d4140edbe",
   "metadata": {},
   "source": [
    "### e. Considérez au moins 5 paires de mots anglais, certains proches par leurs sens, d’autres plus éloignés. Pour chaque paire, calculez la similarité entre les deux mots. Veuillez indiquer si les similarités obtenues correspondent à vos intuitions sur la proximité des sens des mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e8e48ac1c3d37d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:30:13.852502Z",
     "start_time": "2025-05-06T14:30:13.844755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': 0.6511\n",
      "Similarity between 'bad' and 'bunny': 0.1326\n",
      "Similarity between 'good' and 'bunny': 0.1150\n",
      "Similarity between 'plane' and 'tower': 0.2549\n",
      "Similarity between 'duc' and 'duchesse': 0.3245\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('king', 'queen'),\n",
    "    ('bad', 'bunny'),\n",
    "    ('good', 'bunny'),\n",
    "    ('plane', 'tower'),\n",
    "    ('duc', 'duchesse')\n",
    "]\n",
    "\n",
    "for pair in pairs:\n",
    "    word1, word2 = pair\n",
    "    if word1 in w2v_vectors and word2 in w2v_vectors:\n",
    "        similarity = w2v_vectors.similarity(word1, word2)\n",
    "        print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n",
    "    else:\n",
    "        print(f\"One or both words in the pair '{pair}' are not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951989ff91ba049",
   "metadata": {},
   "source": [
    "> TODO answer and/or pick better words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9c32c1020cc2",
   "metadata": {},
   "source": [
    "### f. Pouvez-vous trouver des mots de sens opposés mais qui sont proches selon le modèle ? Comment expliquez-vous cela ? Est-ce une qualité ou un défaut du modèle word2vec ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94799ffa7d27059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:33:42.213054Z",
     "start_time": "2025-05-06T14:33:42.209169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.8092\n"
     ]
    }
   ],
   "source": [
    "similarity = w2v_vectors.similarity('black', 'white')\n",
    "print(f\"Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7e73112415bfb",
   "metadata": {},
   "source": [
    ">TODO answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bab382b5159fd4",
   "metadata": {},
   "source": [
    "### g. En vous aidant de la documentation de Gensim sur KeyedVectors, obtenez les scores du modèle word2vec sur les données de test WordSimilarity-353. Veuillez rappeler en 1-2 phrases comment les différents scores sont calculés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa6ea7a573d85c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T14:36:41.356661Z",
     "start_time": "2025-05-06T14:36:41.141005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.6239\n",
      "Pearson correlation p-value: 0.0000\n",
      "Spearman rank correlation coefficient: 0.6589\n",
      "Spearman rank correlation p-value: 0.0000\n",
      "Ratio of pairs with OOV words: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "word_sim_353_path = datapath('wordsim353.tsv')\n",
    "result = w2v_vectors.evaluate_word_pairs(word_sim_353_path)\n",
    "\n",
    "print(f\"Pearson correlation coefficient: {result[0][0]:.4f}\")\n",
    "print(f\"Pearson correlation p-value: {result[0][1]:.4f}\")\n",
    "print(f\"Spearman rank correlation coefficient: {result[1][0]:.4f}\")\n",
    "print(f\"Spearman rank correlation p-value: {result[1][1]:.4f}\")\n",
    "print(f\"Ratio of pairs with OOV words: {result[2]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8da2fda5367591",
   "metadata": {},
   "source": [
    "> TODO answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec3e208878055c",
   "metadata": {},
   "source": [
    "### h. En vous aidant de la documentation, calculez le score du modèle word2vec sur les données questions-words.txt. Attention, cette évaluation prend une dizaine de minutes, donc il vaut mieux commencer par tester avec un fragment de ce fichier (copier/coller les 100 premières analogies). Expliquez en 1-2 phrases comment ce score est calculé et ce qu’il mesure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18831db6d37e4dd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T15:50:33.851209Z",
     "start_time": "2025-05-06T15:49:03.273206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 0.7401\n",
      "Number of sections: 15\n",
      "Evaluation time: 170.53 seconds\n",
      "\n",
      "Section-wise results:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSection-wise results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m section_name, section_result \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msection_result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m accuracy (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msection_result[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m correct out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msection_result[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m total)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import time\n",
    "\n",
    "analogies_path = datapath('questions-words.txt')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "result = w2v_vectors.evaluate_word_analogies(analogies_path)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Total accuracy: {result[0]:.4f}\")\n",
    "print(f\"Number of sections: {len(result[1])}\")\n",
    "print(f\"Evaluation time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nSection-wise results:\")\n",
    "for section_name, section_result in result[1]:\n",
    "    print(f\"Section '{section_name}': {section_result[0]:.4f} accuracy ({section_result[1]} correct out of {section_result[2]} total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b61a019db22f42",
   "metadata": {},
   "source": [
    "## 2. Entraîner deux nouveaux modèles word2vec à partir de deux corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc3d1cc321d9c1",
   "metadata": {},
   "source": [
    "### a. En utilisant gensim.downloader (voir question 1) récupérez le corpus qui contient les 108 premiers caractères de Wikipédia (en anglais) avec la commande : corpus = api.load('text8'). Combien de phrases et de mots (tokens) possède ce corpus ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f64562501f2e437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 31.6/31.6MB downloaded\n",
      "Nombre de phrases : 1701\n",
      "Nombre de mots (tokens) : 17005207\n"
     ]
    }
   ],
   "source": [
    "corpus = api.load('text8')\n",
    "sentences = list(corpus)\n",
    "\n",
    "num_sentences = len(sentences)\n",
    "num_tokens = sum(len(sentence) for sentence in sentences)\n",
    "\n",
    "print(f\"Nombre de phrases : {num_sentences}\")\n",
    "print(f\"Nombre de mots (tokens) : {num_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b591ad",
   "metadata": {},
   "source": [
    "### b. Entraînez un nouveau modèle word2vec sur ce nouveau corpus (voir la documentation de Word2vec). Si nécessaire, procédez progressivement, en commençant par utiliser 1% du corpus, puis 10%, etc., pour contrôler le temps nécessaire.\n",
    "- Veuillez indiquer la dimension choisie pour le embedding de ce nouveau modèle.\n",
    "- Combien de temps prend l’entraînement sur le corpus total ?\n",
    "- Quelle est la taille (en Mo) du modèle word2vec résultant ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f05a5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4cc87605b005171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement avec 1% du corpus :\n",
      "Temps d'entraînement : 1.57 secondes\n",
      "Taille du modèle : 3.18 Mo\n",
      "\n",
      "Entraînement avec 10% du corpus :\n",
      "Temps d'entraînement : 18.05 secondes\n",
      "Taille du modèle : 15.41 Mo\n",
      "\n",
      "Entraînement avec 100% du corpus :\n",
      "Temps d'entraînement : 221.69 secondes\n",
      "Taille du modèle : 56.47 Mo\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "corpus_sizes = [0.01, 0.1, 1.0]\n",
    "for size in corpus_sizes:\n",
    "    print(f\"\\nEntraînement avec {int(size * 100)}% du corpus :\")\n",
    "    subset = sentences[:int(len(sentences) * size)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = Word2Vec(sentences=subset, vector_size=embedding_dim, window=5, min_count=5, sg=1, epochs=10)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    model_file = f\"text8_word2vec_{int(size * 100)}.model\"\n",
    "    model.save(model_file)\n",
    "    \n",
    "    model_size_mb = os.path.getsize(model_file) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Temps d'entraînement : {training_time:.2f} secondes\")\n",
    "    print(f\"Taille du modèle : {model_size_mb:.2f} Mo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c5caa8",
   "metadata": {},
   "source": [
    "### c. Mesurez la qualité de ce modèle comme en (1g) et (1h). Ce modèle est-il meilleur que celui entraîné sur Google News ? Quelle est selon vous la raison de la différence ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24342035",
   "metadata": {},
   "source": [
    "### d. Téléchargez maintenant le corpus quatre fois plus grand constitué de la concaténation du corpus text8 et des dépêches économiques de Reuters fourni par l’enseignant et appelé wikipedia_augmented.zip (à décompresser en un fichier ‘.dat’ de 413 Mo). Entraînez un nouveau modèle word2vec sur ce corpus, en précisant la dimension choisie pour les embeddings.\n",
    "- Utilisez la classe Text8Corpus() pour charger ce corpus, ce qui fera automatiquement la\n",
    "tokenisation et la segmentation en phrases.\n",
    "- Combien de temps prend l’entraînement ?\n",
    "- Quelle est la taille (en Mo) du modèle word2vec résultant ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ea6dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Text8Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8151986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'entraînement : 1054.53 secondes\n",
      "Taille du modèle : 3.71 Mo\n"
     ]
    }
   ],
   "source": [
    "corpus_path = \"wikipedia_augmented.dat\"\n",
    "corpus = Text8Corpus(corpus_path)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "start_time = time.time()\n",
    "model = Word2Vec(sentences=corpus, vector_size=embedding_dim, window=5, min_count=5, sg=1, epochs=10)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "model_file = \"wikipedia_augmented_word2vec.model\"\n",
    "model.save(model_file)\n",
    "\n",
    "model_size_mb = os.path.getsize(model_file) / (1024 * 1024)\n",
    "\n",
    "print(f\"Temps d'entraînement : {training_time:.2f} secondes\")\n",
    "print(f\"Taille du modèle : {model_size_mb:.2f} Mo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4eeb259fd1f9a",
   "metadata": {},
   "source": [
    "### e. Testez ce modèle comme en (1g) et (1h). Est-il meilleur que le précédent ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52768fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
