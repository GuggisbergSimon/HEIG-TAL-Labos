{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\"/>\n",
    "\n",
    "# Cours TAL - Laboratoire 6\n",
    "# Trois méthodes de désambiguïsation lexicale\n",
    "\n",
    "**Objectif**\n",
    "\n",
    "L'objectif de ce laboratoire est d'implémenter et de comparer plusieurs méthodes de désambiguïsation lexicale (en anglais, *Word Sense Disambiguation* ou WSD).  Vous utiliserez un corpus avec plusieurs milliers de phrases, chaque phrase contenant une occurrence du mot anglais *interest* annotée avec le sens que ce mot possède dans la phrase respective.  Les trois méthodes sont les suivantes (elles seront détaillées par la suite) :\n",
    "\n",
    "* Algorithme de Lesk simplifié.\n",
    "* Utilisation de word2vec.\n",
    "* Classification supervisée utilisant des traits lexicaux.\n",
    "\n",
    "Les deux premières méthodes n'utilisent pas l'apprentissage automatique.  Elles fonctionnent selon le même principe : comparer le contexte d'une occurrence de *interest* avec chacune des définitions des sens (*synsets*) et choisir la définition la plus proche du contexte.  L'algorithme de Lesk définit la proximité comme le nombre de mots en commun, alors que word2vec la calcule comme la similarité de vecteurs.  La dernière méthode vise à classifier les occurrences de *interest*, les sens étant les classes, et les attributs étant les mots du contexte (apprentissage supervisé)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyse des données\n",
    "\n",
    "Téléchargez le corpus *interest* depuis le [site du Prof. Ted Pedersen](http://www.d.umn.edu/~tpederse/data.html) (il se trouve en bas de sa page web).  Téléchargez l'archive ZIP marquée *original format without POS tags* et extrayez le fichier `interest-original.txt`.  Téléchargez également le fichier `README.int.txt` indiqué à la ligne au-dessus. Veuillez répondre brièvement aux questions suivantes :\n",
    "\n",
    "a. Quelles sont les URL du fichier ZIP et celle du fichier `README.int.txt` ?\n",
    "\n",
    "b. Quel est le format du fichier `interest-original.txt` et comment sont annotés les sens de *interest* ?\n",
    "\n",
    "c. Est-ce qu'il y a aussi des occurrences au pluriel (*interests*) à traite ?\n",
    "\n",
    "d. Comment sont annotées les phrases qui contiennent plusieurs occurrences du mot *interest* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a. https://www.d.umn.edu/~tpederse/Data/interest-original.nopos.tar.gz & https://www.d.umn.edu/~tpederse/Data/README.int.txt\n",
    "\n",
    "> b. Le fichier interest-original.txt est un fichier texte brut. Les occurrences du mot \"interest\" sont annotées avec un suffixe de type \"_X\", où X est un nombre représentant le sens spécifique du mot. Par exemple, \"interest_6\" et \"interest_5\" indiquent deux sens différents.\n",
    "\n",
    "> c. Oui il y a des occurrences au pluriel (\"interests\") dans le fichier, également annotées avec un suffixe pour indiquer leur sens.\n",
    "\n",
    "> d. Les phrases avec plusieurs occurrences de \"interest\" ou \"interests\" annotent chaque occurrence séparément, chaque mot ayant son propre identifiant de sens par exemple \"interest_6\" et \"interest_5\" dans la même phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1e.** D'après le fichier `README.int.txt`, quelles sont les définitions des six sens de *interest* annotés dans les données et quelles sont leurs fréquences ? Vous pouvez copier/coller l'extrait de `README`ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Sense 1 : Readiness to give attention\n",
    "\n",
    " - Sense 2 : Quality of causing attention to be given to\n",
    "\n",
    " - Sense 3 : Activity, etc. that one gives attention to\n",
    "\n",
    " - Sense 4 : Advantage, advancement, or favor\n",
    "\n",
    " - Sense 5 : A share in a company or business\n",
    "\n",
    " - Sense 6 : Money paid for the use of money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1f.** De quel dictionnaire viennent les sens précédents ? Où peut-on le consulter en ligne ?  Veuillez aligner les définitions du dictionnaire avec les six sens annotés en écrivant par exemple `Sense 3 = \"an activity that you enjoy doing or a subject that you enjoy studying\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les six sens annotés du mot interest dans le corpus proviennent de la première édition électronique du Longman Dictionary of Contemporary English (LDOCE). Ce dictionnaire est accessible en ligne à l'adresse suivante : https://www.ldoceonline.com/\n",
    "\n",
    "- Sense 1 = \"the feeling of wanting to give your attention to something or of wanting to be involved with and to discover more about something\"\n",
    "\n",
    "- Sense 2 = \"the quality that makes you think that something is interesting\"\n",
    "\n",
    "- Sense 3 = \"an activity or subject that you enjoy and spend time doing or studying\"\n",
    "\n",
    "- Sense 4 = \"something that brings advantages to someone or something; a benefit or advantage\"\n",
    "\n",
    "- Sense 5 = \"a legal share in a business, property, or other financial venture\"\n",
    "\n",
    "- Sense 6 = \"money that is charged by a bank or other financial organization for borrowing money, or paid to someone for investing money\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1g.** En consultant [WordNet en ligne](http://wordnetweb.princeton.edu/perl/webwn), trouvez les définitions des synsets  pour le **nom commun** *interest*.  Combien de synsets y a-t-il ?  Veuillez indiquer comme avant la **définition** de chaque synset pour chacun des six sens ci-dessus (au besoin, fusionner ou ignorer des synsets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sense 1 : Readiness to give attention\n",
    "\n",
    "Définition WordNet : a sense of concern with and curiosity about someone or something\n",
    "\n",
    "- Sense 2 : Quality of causing attention to be given to\n",
    "\n",
    "Définition WordNet : the power of attracting or holding one's attention (because it is unusual or exciting etc.)\n",
    "\n",
    "- Sense 3 : Activity, etc. that one gives attention to\n",
    "\n",
    "Définition WordNet : a diversion that occupies one's time and thoughts (usually pleasantly)\n",
    "\n",
    "- Sense 4 : Advantage, advancement or favor\n",
    "\n",
    "Définition WordNet : a reason for wanting something done\n",
    "\n",
    "- Sense 5 : A share in a company or business\n",
    "\n",
    "Définition WordNet : a right or legal share of something; a financial involvement with something\n",
    "\n",
    "- Sense 6 : Money paid for the use of money\n",
    "\n",
    "Définition WordNet : a fixed charge for borrowing money; usually a percentage of the amount borrowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1h.** Définissez (manuellement, ou avec quelques lignes de code) une liste nommée `senses1` avec les mots des définitions du README, en supprimant les stopwords (p.ex. les mots < 4 lettres).  Affichez la liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['readiness', 'give', 'attention', 'quality', 'causing', 'attention', 'given', 'activity,', 'gives', 'attention', 'advantage,', 'advancement', 'favor', 'share', 'company', 'business', 'money', 'paid', 'money']\n"
     ]
    }
   ],
   "source": [
    "raw_definitions = [\n",
    "    \"readiness to give attention\",\n",
    "    \"quality of causing attention to be given to\",\n",
    "    \"activity, etc. that one gives attention to\",\n",
    "    \"advantage, advancement or favor\",\n",
    "    \"a share in a company or business\",\n",
    "    \"money paid for the use of money\"\n",
    "]\n",
    "\n",
    "stopwords = {\"to\", \"of\", \"and\", \"or\", \"for\", \"in\", \"the\", \"a\", \"be\", \"etc.\", \"that\", \"one\", \"is\", \"on\", \"with\"}\n",
    "\n",
    "senses1 = [\n",
    "    word.lower() \n",
    "    for definition in raw_definitions \n",
    "    for word in definition.split() \n",
    "    if len(word) >= 4 and word.lower() not in stopwords\n",
    "]\n",
    "\n",
    "print(senses1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1i.** En combinant les définitions obtenues aux points (4) et (5) ci-dessus, construisez une liste nommée `senses2` avec pour chacun des sens de *interest* une liste de **mots-clés** correspondants.  Vous pouvez concaténer les définitions, puis écrire des instructions en Python pour extraire les mots (uniques).  Respectez l'ordre des sens données par `README`, et à la fin affichez `senses2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:21:02.419539Z",
     "start_time": "2025-05-15T12:21:02.413599Z"
    }
   },
   "source": [
    "definitions_combined = [\n",
    "    # Sense 1\n",
    "    \"readiness to give attention wanting to know or learn about something or someone an activity that you enjoy doing or a subject that you enjoy studying\",\n",
    "    \n",
    "    # Sense 2\n",
    "    \"quality of causing attention to be given to the quality of attracting interest or attention\",\n",
    "    \n",
    "    # Sense 3\n",
    "    \"activity that one gives attention to a real or apparent aim or purpose a topic or subject of concern or curiosity\",\n",
    "    \n",
    "    # Sense 4\n",
    "    \"advantage advancement or favor advantage or profit interest in the success of a business\",\n",
    "    \n",
    "    # Sense 5\n",
    "    \"a share in a company or business a legal concern or right a right to receive part of the proceeds of a loan or investment\",\n",
    "    \n",
    "    # Sense 6\n",
    "    \"money paid for the use of money a charge for the use of money usually a percentage of the amount borrowed money paid regularly at a particular rate for the use of money lent\"\n",
    "]\n",
    "\n",
    "# Fonction pour extraire mots > 3 lettres et uniques\n",
    "def extract_keywords(text):\n",
    "    # Met en minuscule et extrait mots alpha >= 4 lettres\n",
    "    words = re.findall(r'\\b[a-z]{4,}\\b', text.lower())\n",
    "    return list(set(words))\n",
    "\n",
    "senses2 = [extract_keywords(defn) for defn in definitions_combined]\n",
    "\n",
    "print(senses2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['attention', 'activity', 'someone', 'that', 'know', 'give', 'learn', 'doing', 'subject', 'studying', 'enjoy', 'readiness', 'about', 'wanting', 'something'], ['attention', 'causing', 'given', 'quality', 'interest', 'attracting'], ['curiosity', 'attention', 'topic', 'concern', 'that', 'purpose', 'real', 'apparent', 'subject', 'gives', 'activity'], ['advantage', 'advancement', 'interest', 'favor', 'profit', 'success', 'business'], ['part', 'concern', 'receive', 'share', 'company', 'loan', 'proceeds', 'investment', 'right', 'business', 'legal'], ['money', 'percentage', 'amount', 'particular', 'regularly', 'lent', 'usually', 'rate', 'charge', 'borrowed', 'paid']]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1j.** Chargez les données depuis `interest-original.txt` dans une liste appelée `sentences` qui contient pour chaque phrase la liste des mots (sans les séparateurs *$$* et *===...*).  Ces phrases sont-elles déjà tokenisées en mots ?  Sinon, faites-le.  À ce stade, ne modifiez pas encore les occurrences annotées *interest(s)\\_X*.  Comptez le nombre total de phrases et affichez-en trois au hasard."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:20:03.165522Z",
     "start_time": "2025-05-15T12:20:03.150789Z"
    }
   },
   "source": [
    "filepath = \"interest-original.txt\"\n",
    "\n",
    "sentences = []\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "raw_sentences = content.split('$$')\n",
    "\n",
    "for sent in raw_sentences:\n",
    "    sent = sent.strip()\n",
    "    if sent == \"\" or sent.startswith('='):  # Ignorer les lignes de séparation\n",
    "        continue\n",
    "    tokens = sent.split()\n",
    "    sentences.append(tokens)\n",
    "\n",
    "print(\"Il y a {} phrases.\\nEn voici 3 au hasard :\".format(len(sentences)))\n",
    "print(sentences[151:154])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 1228 phrases.\n",
      "En voici 3 au hasard :\n",
      "[['brunswick', 'also', 'has', 'interests_5', 'in', 'defense', 'and', 'aerospace', 'products', '.'], ['market', 'analysts', 'said', 'that', 'continued', 'declines', 'and', 'volatility', 'in', 'the', 'stock', 'market', 'contributed', 'generally', 'to', 'enhanced', 'investor', 'interest_1', 'in', 'precious', 'metals', ',', 'but', 'that', 'the', 'british', 'political', 'news', 'was', 'the', 'main', 'factor', 'boosting', 'prices', ':', 'chancellor', 'of', 'the', 'exchequer', 'nigel', 'lawson', 'resigned', '.'], ['but', 'he', 'also', 'noted', 'that', 'if', 'uncertainty', 'continues', 'in', 'the', 'stock', 'market', ',', 'that', 'might', 'create', 'some', 'investor', 'interest_1', 'in', 'precious', 'metals', '.']]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithme de Lesk simplifié\n",
    "\n",
    "**2a.** Définissez une fonction `wsd_lesk(senses, sentence)` qui prend deux arguments : une liste de listes de mots-clés (comme `senses1` et `senses2` ci-dessus) et une phrase avec une occurrence annotée de *interest* ou *interests*, et qui retourne l'index du sens le plus probable (entre 1 et 6) selon l'algorithme de Lesk.  Cet algorithme choisit le sens qui a le maximum de mots en commun avec le contexte de *interest*.  Vous pouvez choisir vous-mêmes la taille de ce voisinage (`window_size`).  En cas d'égalité entre deux sens, tirer la réponse au sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsd_lesk(senses, sentence, window_size=5):\n",
    "    interest_indices = [i for i, w in enumerate(sentence) if 'interest' in w.lower()]\n",
    "    \n",
    "    if not interest_indices:\n",
    "        raise ValueError(\"La phrase ne contient pas d'occurrence de 'interest' annoté.\")\n",
    "    \n",
    "    idx = interest_indices[0]\n",
    "    \n",
    "    # Extraire le contexte autour de 'interest' dans la fenêtre définie\n",
    "    start = max(0, idx - window_size)\n",
    "    end = min(len(sentence), idx + window_size + 1)\n",
    "    context_words = set(w.lower().strip('.,;:\"\\'()[]') for w in sentence[start:end] if 'interest' not in w.lower())\n",
    "    \n",
    "    # Calculer le score de chevauchement pour chaque sens\n",
    "    scores = []\n",
    "    for sense_idx, sense_words in enumerate(senses, start=1):\n",
    "        overlap = len(context_words.intersection(sense_words))\n",
    "        scores.append((sense_idx, overlap))\n",
    "    \n",
    "    max_score = max(scores, key=lambda x: x[1])[1]\n",
    "    \n",
    "    # Extraire tous les sens avec le score maximum (possibilité d'égalité)\n",
    "    best_senses = [sense for sense, score in scores if score == max_score]\n",
    "    \n",
    "    chosen_sense = random.choice(best_senses)\n",
    "    \n",
    "    return chosen_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Définissez maintenant une fonction `evaluate_wsd(fct_name, senses, sentences)` qui prend en paramètre le nom de la méthode de similarité (pour commencer : `wsd_lesk`) ainsi que la liste des mots-clés par sens, et la liste de phrases, et qui retourne le score de la méthode de similarité.  Ce score sera tout simplement le pourcentage de réponses correctes (sens trouvé identique au sens annoté)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:19:49.306348Z",
     "start_time": "2025-05-15T12:19:49.299930Z"
    }
   },
   "source": [
    "def evaluate_wsd(fct_name, senses, sentences):\n",
    "    correct = 0\n",
    "    total = len(sentences)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        interest_word = next((w for w in sentence if 'interest_' in w.lower() or 'interests_' in w.lower()), None)\n",
    "        if interest_word is None:\n",
    "            continue\n",
    "        \n",
    "        true_sense = int(interest_word.split('_')[-1].split('/')[0])  # en cas de POS tags\n",
    "        \n",
    "        pred_sense = fct_name(senses, sentence)\n",
    "        \n",
    "        if pred_sense == true_sense:\n",
    "            correct += 1\n",
    "    \n",
    "    score = (correct / total) * 100 if total > 0 else 0\n",
    "    return score"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** En fixant au mieux la taille de la fenêtre autour de *interest*, quel est le meilleur score de la méthode de Lesk simplifiée ?  Quelle liste de sens conduit à de meilleurs scores, `senses1` ou `senses2` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window=1: senses1 score=4.48%, senses2 score=21.91%\n",
      "Window=2: senses1 score=5.21%, senses2 score=23.05%\n",
      "Window=3: senses1 score=6.35%, senses2 score=23.78%\n",
      "Window=4: senses1 score=5.78%, senses2 score=23.86%\n",
      "Window=5: senses1 score=5.86%, senses2 score=24.43%\n",
      "Window=6: senses1 score=6.03%, senses2 score=23.53%\n",
      "Window=7: senses1 score=6.35%, senses2 score=24.10%\n",
      "Window=8: senses1 score=5.70%, senses2 score=24.84%\n",
      "Window=9: senses1 score=7.00%, senses2 score=22.96%\n",
      "Window=10: senses1 score=5.86%, senses2 score=23.05%\n",
      "\n",
      "Meilleur score : 24.84% avec fenêtre 8 et senses2\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_window = 0\n",
    "best_senses = None\n",
    "\n",
    "for window in range(1, 11):\n",
    "    def lesk_with_window(senses, sentence):\n",
    "        return wsd_lesk(senses, sentence, window_size=window)\n",
    "    \n",
    "    score1 = evaluate_wsd(lesk_with_window, senses1, sentences)\n",
    "    score2 = evaluate_wsd(lesk_with_window, senses2, sentences)\n",
    "    \n",
    "    print(f\"Window={window}: senses1 score={score1:.2f}%, senses2 score={score2:.2f}%\")\n",
    "    \n",
    "    if score1 > best_score:\n",
    "        best_score = score1\n",
    "        best_window = window\n",
    "        best_senses = \"senses1\"\n",
    "    if score2 > best_score:\n",
    "        best_score = score2\n",
    "        best_window = window\n",
    "        best_senses = \"senses2\"\n",
    "\n",
    "print(f\"\\nMeilleur score : {best_score:.2f}% avec fenêtre {best_window} et {best_senses}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilisation de word2vec pour la similarité contexte vs. synset\n",
    "\n",
    "**3a.** En réutilisant une partie du code de `wsd_lesk`, veuillez maintenant définir une fonction `wsd_word2vec(senses, sentence)` qui choisit le sens en utilisant la similarité **word2vec** étudiée dans le labo précédent. \n",
    "* Vous pouvez chercher dans la [documentation des KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html) comment calculer directement la similarité entre deux listes de mots.\n",
    "* Comme `wsd_lesk`, la nouvelle fonction `wsd_word2vec` prend en argument une liste de listes de mots-clés par sens (comme `senses1` et `senses2` ci-dessus), et une phrase avec une occurrence annotée de *interest* ou *interests*.\n",
    "* La fonction retourne le numéro du sens le plus probable selon la similarité word2vec entre les mots du sens et ceux du voisinage de *interest*.  En cas d'égalité, tirer le sens au sort.\n",
    "* Vous pouvez régler la taille du voisinage (`window_size`) par l'expérimentation."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:09:42.397187Z",
     "start_time": "2025-05-15T12:09:42.391495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model is then located at home directory, adjust as needed.\n",
    "import os\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"gensim-data\", \"word2vec-google-news-300\", \"word2vec-google-news-300.gz\")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:11:12.858277Z",
     "start_time": "2025-05-15T12:09:57.902146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Takes ~1min to load the model\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "wv_model = gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=True)  # C bin format"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:17:17.681865Z",
     "start_time": "2025-05-15T12:17:17.622326Z"
    }
   },
   "source": [
    "def wsd_word2vec(senses, sentence, window_size=5):\n",
    "    interest_indices = [i for i, w in enumerate(sentence) if 'interest' in w.lower()]\n",
    "\n",
    "    if not interest_indices:\n",
    "        raise ValueError(\"La phrase ne contient pas d'occurrence de 'interest' annoté.\")\n",
    "\n",
    "    idx = interest_indices[0]\n",
    "\n",
    "    # Extraire le contexte autour de 'interest' dans la fenêtre définie\n",
    "    start = max(0, idx - window_size)\n",
    "    end = min(len(sentence), idx + window_size + 1)\n",
    "    context_words = [w.lower().strip('.,;:\"\\'()[]') for w in sentence[start:end] if 'interest' not in w.lower()]\n",
    "\n",
    "    context_words = [w for w in context_words if w in wv_model.key_to_index]\n",
    "\n",
    "    if not context_words:\n",
    "        return random.randint(1, len(senses))\n",
    "\n",
    "    # Calculer le score de chevauchement pour chaque sens\n",
    "    scores = []\n",
    "    for sense_idx, sense_words in enumerate(senses, start=1):\n",
    "        valid_sense_words = [w for w in sense_words if w in wv_model.key_to_index]\n",
    "\n",
    "        if not valid_sense_words:\n",
    "            scores.append((sense_idx, 0))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            similarity = wv_model.n_similarity(context_words, valid_sense_words)\n",
    "            scores.append((sense_idx, similarity))\n",
    "        except (KeyError, ZeroDivisionError):\n",
    "            scores.append((sense_idx, 0))\n",
    "\n",
    "    max_score = max(scores, key=lambda x: x[1])[1]\n",
    "\n",
    "    # Extraire tous les sens avec le score maximum (possibilité d'égalité)\n",
    "    best_senses = [sense for sense, score in scores if score == max_score]\n",
    "\n",
    "    chosen_sense = random.choice(best_senses)\n",
    "\n",
    "    return chosen_sense"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Appliquez maintenant la même méthode `evaluate_wsd` avec la fonction `wsd_word2vec` (en cherchant une bonne valeur de la taille de la fenêtre) et affichez le score de la similarité word2vec.  Comment se compare-t-il avec le score précédent (Lesk) ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:22:11.966168Z",
     "start_time": "2025-05-15T12:21:33.395350Z"
    }
   },
   "source": [
    "best_score = 0\n",
    "best_window = 0\n",
    "best_senses = None\n",
    "\n",
    "for window in range(1, 11):\n",
    "    def word2vec_with_window(senses, sentence):\n",
    "        return wsd_word2vec(senses, sentence, window_size=window)\n",
    "\n",
    "    score1 = evaluate_wsd(word2vec_with_window, senses1, sentences)\n",
    "    score2 = evaluate_wsd(word2vec_with_window, senses2, sentences)\n",
    "\n",
    "    print(f\"Window={window}: senses1 score={score1:.2f}%, senses2 score={score2:.2f}%\")\n",
    "\n",
    "    if score1 > best_score:\n",
    "        best_score = score1\n",
    "        best_window = window\n",
    "        best_senses = \"senses1\"\n",
    "    if score2 > best_score:\n",
    "        best_score = score2\n",
    "        best_window = window\n",
    "        best_senses = \"senses2\"\n",
    "\n",
    "print(f\"\\nMeilleur score : {best_score:.2f}% avec fenêtre {best_window} et {best_senses}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window=1: senses1 score=1.63%, senses2 score=53.34%\n",
      "Window=2: senses1 score=2.36%, senses2 score=55.62%\n",
      "Window=3: senses1 score=1.55%, senses2 score=57.41%\n",
      "Window=4: senses1 score=1.55%, senses2 score=57.82%\n",
      "Window=5: senses1 score=1.55%, senses2 score=58.47%\n",
      "Window=6: senses1 score=1.47%, senses2 score=59.45%\n",
      "Window=7: senses1 score=0.90%, senses2 score=57.82%\n",
      "Window=8: senses1 score=1.47%, senses2 score=57.57%\n",
      "Window=9: senses1 score=0.98%, senses2 score=56.68%\n",
      "Window=10: senses1 score=1.14%, senses2 score=55.62%\n",
      "\n",
      "Meilleur score : 59.45% avec fenêtre 6 et senses2\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Le score est significativement meilleur, doublé, passant de 24.84% avec Lesk à 59.45% avec word2vec\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification supervisée avec des traits lexicaux\n",
    "Vous entraînerez maintenant des classifieurs pour prédire le sens d'une occurrence dans une phrase.  Le premier but sera de transformer chaque phrase en un ensemble d'attributs pour formater les données en vue des expériences de classification.\n",
    "\n",
    "Veuillez utiliser le classifieur `NaiveBayesClassifier` fourni par NLTK.  Le mode d'emploi se trouve dans le [Chapitre 6, sections 1.1-1.3](https://www.nltk.org/book/ch06.html) du livre NLTK.  Consultez-le attentivement pour trouver comment formater les données.  De plus, il faudra séparer les données en sous-ensembles d'entraînement et de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vous propose de nommer les attributs `word-k`, ..., `word-2`, `word-1`, `word+1`, `word+2`, ..., `word+k` (fenêtre de taille `2*k` autour de *interest*).  Leurs valeurs sont les mots observés aux emplacements respectifs, ou `NONE` si la position dépasse l'étendue de la phrase.  Vous ajouterez un attribut nommé `word0` qui est l'occurrence du mot *interest* au singulier ou au pluriel.  \n",
    "\n",
    "Pour chaque occurrence de *interest*, vous devrez donc créer la représentation suivante (où `6` est le numéro du sens, essentiel pour l'entraînement, mais à cacher lors de l'évaluation) :\n",
    "```\n",
    "[{'word-1': 'in', 'word+1': 'rates', 'word-2': 'declines', 'word+2': 'NONE', 'word0': 'interest'}, 6]\n",
    "```\n",
    "\n",
    "**4a.** En partant de la liste des phrases appelée `sentences` préparée plus haut, veuillez générer la liste avec toutes les représentation, appelée `items_with_features`.  Vous pouvez vous aider du livre NLTK."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:25:28.447359Z",
     "start_time": "2025-05-15T14:25:28.436521Z"
    }
   },
   "source": [
    "def extract_features(sentence, target_idx, window_size=2):\n",
    "    features = {}\n",
    "\n",
    "    target_word = sentence[target_idx].lower()\n",
    "    # strips suffixes denoting sense\n",
    "    if '_' in target_word:\n",
    "        base_word = target_word.split('_')[0]\n",
    "    else:\n",
    "        base_word = target_word\n",
    "    features['word0'] = base_word\n",
    "\n",
    "    # window iteration\n",
    "    for i in range(1, window_size + 1):\n",
    "        # words before\n",
    "        pos = target_idx - i\n",
    "        if pos >= 0:\n",
    "            features[f'word-{i}'] = sentence[pos].lower()\n",
    "        else:\n",
    "            features[f'word-{i}'] = 'NONE'\n",
    "\n",
    "        # words after\n",
    "        pos = target_idx + i\n",
    "        if pos < len(sentence):\n",
    "            features[f'word+{i}'] = sentence[pos].lower()\n",
    "        else:\n",
    "            features[f'word+{i}'] = 'NONE'\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def items_features_per_window(window_size):\n",
    "    items_features = []\n",
    "    for sentence in sentences:\n",
    "        # find occurrences of interest/interests with annotation\n",
    "        for i, word in enumerate(sentence):\n",
    "            if 'interest_' in word.lower() or 'interests_' in word.lower():\n",
    "                # Extract the sense and feature\n",
    "                sense = int(word.split('_')[-1])\n",
    "                features = extract_features(sentence, i, window_size)\n",
    "                items_features.append([features, sense])\n",
    "    return items_features"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:25:30.487814Z",
     "start_time": "2025-05-15T14:25:30.475485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "window_size = 2\n",
    "items_with_features = items_features_per_window(window_size)\n",
    "\n",
    "print(len(items_with_features))\n",
    "print(items_with_features[151:154])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1228\n",
      "[[{'word0': 'interests', 'word-1': 'has', 'word+1': 'in', 'word-2': 'also', 'word+2': 'defense'}, 5], [{'word0': 'interest', 'word-1': 'investor', 'word+1': 'in', 'word-2': 'enhanced', 'word+2': 'precious'}, 1], [{'word0': 'interest', 'word-1': 'investor', 'word+1': 'in', 'word-2': 'some', 'word+2': 'precious'}, 1]]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.** Veuillez séparer les données aléatoirement en 80% pour l'entraînement et 20%  pour l'évaluation.  Veuillez faire une division stratifiée : les deux sous-ensembles doivent contenir les mêmes proportions de sens que l'ensemble de départ.  Ils seront appelés `iwf_train` et `iwf_test`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:35:37.940590Z",
     "start_time": "2025-05-15T12:35:37.928441Z"
    }
   },
   "source": [
    "from random import shuffle"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:25:36.553180Z",
     "start_time": "2025-05-15T14:25:36.548148Z"
    }
   },
   "source": [
    "def split_train_test(split_point=0.8):\n",
    "    train = []\n",
    "    test  = []\n",
    "    # Group items by sense\n",
    "    sense_groups = {}\n",
    "    for item in items_with_features:\n",
    "        sense = item[1]\n",
    "        if sense not in sense_groups:\n",
    "            sense_groups[sense] = []\n",
    "        sense_groups[sense].append(item)\n",
    "\n",
    "    for sense, items in sense_groups.items():\n",
    "        # Shuffle items by sense\n",
    "        items_copy = items.copy()\n",
    "        shuffle(items_copy)\n",
    "\n",
    "        # Split into train and test\n",
    "        split_idx = int(len(items_copy) * split_point)\n",
    "        train.extend(items_copy[:split_idx])\n",
    "        test.extend(items_copy[split_idx:])\n",
    "\n",
    "    # Additional shuffle post split\n",
    "    shuffle(train)\n",
    "    shuffle(test)\n",
    "    return train, test"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:25:38.140259Z",
     "start_time": "2025-05-15T14:25:38.132886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "iwf_train, iwf_test = split_train_test()\n",
    "\n",
    "print(len(iwf_train), ' ', len(iwf_test))\n",
    "print(iwf_test[:2], iwf_test[-2:])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980   248\n",
      "[[{'word0': 'interest', 'word-1': 'NONE', 'word+1': 'rates', 'word-2': 'NONE', 'word+2': 'can'}, 6], [{'word0': 'interest', 'word-1': 'and', 'word+1': '.', 'word-2': 'principal', 'word+2': 'NONE'}, 6]] [[{'word0': 'interest', 'word-1': 'u.s.', 'word+1': 'rates', 'word-2': 'that', 'word+2': 'are'}, 6], [{'word0': 'interest', 'word-1': 'no', 'word+1': ';', 'word-2': 'has', 'word+2': 'again'}, 5]]\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c.** Veuillez créer une instance de `NaiveBayesClassifier`, l'entraîner sur `iwf_train` et la tester sur `iwf_test` (voir la documentation NLTK).  En expérimentant avec différentes largeurs de fenêtres, quel est le meilleur score que vous obtenez (avec la fonction `accuracy` de NLTK) sur l'ensemble de test ?  Comment se compare-t-il avec les précédents ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:30:53.081783Z",
     "start_time": "2025-05-15T14:30:53.077399Z"
    }
   },
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "def test_and_train(train, test):\n",
    "    train_set = [(item[0], item[1]) for item in train]\n",
    "    test_set = [(item[0], item[1]) for item in test]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    return classifier, accuracy(classifier, test_set)"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:39:26.995629Z",
     "start_time": "2025-05-15T14:39:26.361541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_acc = 0\n",
    "best_window = 0\n",
    "best_classifier = None\n",
    "best_iwf_test = None\n",
    "\n",
    "for window in range(1, 11):\n",
    "    items_with_features = items_features_per_window(window)\n",
    "    iwf_train, iwf_test = split_train_test()\n",
    "    classifier, acc = test_and_train(iwf_train, iwf_test)\n",
    "    print(f\"Accuracy for window {window:2d} : {acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_window = window\n",
    "        best_classifier = classifier\n",
    "        best_iwf_test = iwf_test\n",
    "\n",
    "print(f\"\\nBest window size: {best_window} with accuracy: {best_acc:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for window  1 : 0.8710\n",
      "Accuracy for window  2 : 0.8548\n",
      "Accuracy for window  3 : 0.8508\n",
      "Accuracy for window  4 : 0.8468\n",
      "Accuracy for window  5 : 0.8105\n",
      "Accuracy for window  6 : 0.8548\n",
      "Accuracy for window  7 : 0.8105\n",
      "Accuracy for window  8 : 0.8185\n",
      "Accuracy for window  9 : 0.7903\n",
      "Accuracy for window 10 : 0.7782\n",
      "\n",
      "Best window size: 1 with accuracy: 0.8710\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Le meilleur score varie pour une fenêtre de 1 ou de 2.\n",
    "> Il est néanmoins bien plus élevé que les deux précédentes méthodes en atteignant 85%-89% tandis que word2vec n'atteignait que 59.45% avec une fenêtre de 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4d.** En utilisant la fonction `show_most_informative_features()`, veuillez afficher les attributs les plus informatifs et commenter le résultat."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T15:03:08.597938Z",
     "start_time": "2025-05-15T15:03:08.590061Z"
    }
   },
   "source": "best_classifier.show_most_informative_features(20)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  word+1 = 'in'                1 : 6      =     46.5 : 1.0\n",
      "                  word-1 = 'other'             3 : 6      =     20.3 : 1.0\n",
      "                  word+1 = 'of'                4 : 6      =     19.9 : 1.0\n",
      "                  word-1 = 'of'                4 : 5      =     11.2 : 1.0\n",
      "                  word-1 = 'and'               6 : 5      =     10.2 : 1.0\n",
      "                  word+1 = '.'                 3 : 6      =     10.1 : 1.0\n",
      "                  word-1 = 'with'              5 : 6      =      9.5 : 1.0\n",
      "                  word-1 = 'own'               4 : 6      =      9.1 : 1.0\n",
      "                  word-1 = 'in'                6 : 5      =      8.7 : 1.0\n",
      "                  word+1 = 'to'                4 : 6      =      8.0 : 1.0\n",
      "                  word-1 = 'any'               1 : 6      =      7.6 : 1.0\n",
      "                  word-1 = 'our'               4 : 6      =      7.1 : 1.0\n",
      "                  word-1 = 'public'            4 : 1      =      6.0 : 1.0\n",
      "                  word-1 = 'the'               6 : 5      =      5.7 : 1.0\n",
      "                  word-1 = 'its'               5 : 1      =      5.7 : 1.0\n",
      "                  word-1 = '%'                 5 : 6      =      5.4 : 1.0\n",
      "                  word+1 = 'on'                6 : 1      =      4.8 : 1.0\n",
      "                   word0 = 'interest'          6 : 3      =      4.8 : 1.0\n",
      "                  word-1 = 'an'                1 : 4      =      4.7 : 1.0\n",
      "                  word+1 = \"''\"                4 : 5      =      4.7 : 1.0\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Nous observons que le mot suivant 'interest/s' qui permet de déterminer son sens de manière la plus tranchée est 'in'.\n",
    "> Celui-ci permet de déterminer qu'il a 46.5 fois plus de chances d'être utilisé dans le sens 1 que dans le sens 6\n",
    ">\n",
    "> À noter que 'interest' se trouve également dans cette liste, en posittion 0, ce qui est attendu.\n",
    "> Ce qui l'est moins est que celui-ci est 4.8 fois plus souvent associé au sens 6 que 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4e.** On souhaite également obtenir les scores pour chaque sens.  Pour ce faire, il faut demander les prédictions une par une au classifieur (voir le [livre NLTK](https://www.nltk.org/book/ch06.html)), et comptabiliser les prédictions correctes pour chaque sens.  Vous pouvez vous inspirer de `evaluate_wsd`, et écrire une fonction `evaluate_wsd_supervised(classifier, items_with_features)`, que vous appliquerez aux donnés `iwf_test`.  Veuillez afficher ces scores."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:41:51.080962Z",
     "start_time": "2025-05-15T14:41:51.047675Z"
    }
   },
   "source": [
    "def evaluate_wsd_supervised(classifier, items_with_features):\n",
    "    total_by_sense = {}\n",
    "    correct_by_sense = {}\n",
    "\n",
    "    # Process each test item\n",
    "    for item in items_with_features:\n",
    "        features = item[0]\n",
    "        true_sense = item[1]\n",
    "        predicted_sense = classifier.classify(features)\n",
    "\n",
    "        # Update counters\n",
    "        if true_sense not in total_by_sense:\n",
    "            total_by_sense[true_sense] = 0\n",
    "            correct_by_sense[true_sense] = 0\n",
    "\n",
    "        total_by_sense[true_sense] += 1\n",
    "        if predicted_sense == true_sense:\n",
    "            correct_by_sense[true_sense] += 1\n",
    "\n",
    "    # Calculate accuracy for each sense\n",
    "    print(\"Accuracy by sense:\")\n",
    "    overall_correct = 0\n",
    "    overall_total = 0\n",
    "\n",
    "    for sense in sorted(total_by_sense.keys()):\n",
    "        accuracy = (correct_by_sense[sense] / total_by_sense[sense]) * 100\n",
    "        print(f\"Sense {sense}: {accuracy:.2f}% ({correct_by_sense[sense]}/{total_by_sense[sense]})\")\n",
    "        overall_correct += correct_by_sense[sense]\n",
    "        overall_total += total_by_sense[sense]\n",
    "\n",
    "    # Overall accuracy\n",
    "    overall_accuracy = (overall_correct / overall_total) * 100\n",
    "    print(f\"\\nOverall accuracy: {overall_accuracy:.2f}% ({overall_correct}/{overall_total})\")\n",
    "    return overall_accuracy\n",
    "\n",
    "\n",
    "evaluate_wsd_supervised(best_classifier, iwf_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by sense:\n",
      "Sense 1: 94.12% (32/34)\n",
      "Sense 2: 0.00% (0/1)\n",
      "Sense 3: 37.50% (3/8)\n",
      "Sense 4: 86.96% (20/23)\n",
      "Sense 5: 83.33% (35/42)\n",
      "Sense 6: 97.86% (137/140)\n",
      "\n",
      "Overall accuracy: 91.53% (227/248)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.53225806451613"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Veuillez recopier ci-dessous, en guise de conclusion, les scores des trois expériences réalisées, pour pouvoir les comparer d'un coup d'oeil.  Quel est le meilleur score obtenu?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> | Méthode   | Score  | Fenêtre |\n",
    "> |-----------|--------|---------|\n",
    "> | Lesk      | 24.84% | 8       |\n",
    "> | Word2vec  | 59.45% | 6       |\n",
    "> | Supervisé | 91.53% | 1-2     |\n",
    "\n",
    "> Le meilleur score obtenu est sans surprise à l'aide de la méthode supervisée.\n",
    "> En effet les 2 autres sont des méthodes automatisées moins élaborées sans le fine tuning que la supervision nous offre."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
